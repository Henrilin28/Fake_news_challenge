{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fake News Challenge\n",
    "\n",
    "http://www.fakenewschallenge.org/\n",
    "\n",
    "https://github.com/FakeNewsChallenge/fnc-1-baseline\n",
    "\n",
    "# 2. Technical Links\n",
    "\n",
    "## NLTK\n",
    "\n",
    "http://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization\n",
    "\n",
    "https://www.dataquest.io/blog/natural-language-processing-with-python/\n",
    "\n",
    "http://www.nltk.org/book/ch03.html\n",
    "\n",
    "\n",
    "## Tensorflow\n",
    "https://www.tensorflow.org/tutorials/recurrent\n",
    "\n",
    "https://www.tensorflow.org/programmers_guide/reading_data\n",
    "\n",
    "## Sklearn\n",
    "\n",
    "http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\n",
    "\n",
    "## Markdown\n",
    "\n",
    "https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet\n",
    "\n",
    "## Keras\n",
    "\n",
    "https://keras.io/getting-started/sequential-model-guide/\n",
    "\n",
    "# 3. Papers\n",
    "\n",
    "https://www.ijcai.org/Proceedings/16/Papers/408.pdf\n",
    "\n",
    "https://www.overleaf.com/5276203cwvkhf#/16617343/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## test code for keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "HOME_DIR='.'\n",
    "FNC_PATH='{}/fnc-1-baseline'.format(HOME_DIR)\n",
    "\n",
    "#must add local path to the FNC utils, so we can import and reuse them\n",
    "sys.path.append(FNC_PATH + '/utils/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\anaconda3\\envs\\mlcv35\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "#Google news vectors link https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/view\n",
    "#We can change it with fastText, but it's 6GB and it's super slow to download\n",
    "\n",
    "W2V_MODEL='{}/model/GoogleNews-vectors-negative300.bin'.format(HOME_DIR)\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "w2vmodel = gensim.models.KeyedVectors.load_word2vec_format(W2V_MODEL, binary=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_data(path=FNC_PATH + '/fnc-1'):\n",
    "    stances = pd.read_csv(path + '/train_stances.csv')\n",
    "    stances.set_index('Body ID', inplace=True)\n",
    "    \n",
    "    bodies = pd.read_csv(path + '/train_bodies.csv')\n",
    "    bodies.set_index('Body ID', inplace=True)\n",
    "    \n",
    "    ds = pd.merge(bodies, stances, how='inner', right_index=True, left_index=True)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_data_split(ds, test_size = 0.2):\n",
    "    train, validation = train_test_split(ds, test_size = test_size)\n",
    "    return train, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples: 39977\n",
      "Test examples: 9995\n",
      "\n",
      "unrelated    29242\n",
      "discuss       7163\n",
      "agree         2908\n",
      "disagree       664\n",
      "Name: Stance, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ds = read_data()\n",
    "\n",
    "train, validation = get_data_split(ds)\n",
    "print (\"Train examples: %d\"%len(train))\n",
    "print (\"Test examples: %d\"%len(validation))\n",
    "\n",
    "print ()\n",
    "print (train['Stance'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Henry\n",
      "[nltk_data]     Lin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Henry\n",
      "[nltk_data]     Lin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Henry Lin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "(39977, 8950)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "\n",
    "def normalize_word(w):\n",
    "    return wnl.lemmatize(w.lower()).lower()\n",
    "\n",
    "def tokenize_sentenses(sentences):\n",
    "    return sentences.apply(lambda s: nltk.word_tokenize(s))\n",
    "\n",
    "def lemmatize_tokens(series):\n",
    "    return series.apply(lambda tokens: [normalize_word(t) for t in tokens])\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    # Removes stopwords from a list of tokens\n",
    "    return words.apply(lambda l: [w for w in l if w not in feature_extraction.text.ENGLISH_STOP_WORDS])\n",
    "\n",
    "def trainTFIDF(corpus, max_ngram):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, max_ngram), lowercase=True, stop_words=\"english\", min_df=10, max_df=100)\n",
    "    vectorizer.fit(corpus)\n",
    "    \n",
    "    return vectorizer\n",
    "\n",
    "def encode_pos(word):\n",
    "    return ['_'.join(x) for x in nltk.pos_tag(word)]\n",
    "\n",
    "def doc2vec(terms):\n",
    "    return np.mean([w2vmodel[w] if w in w2vmodel.vocab else np.zeros(300) for w in terms], axis=0).tolist()\n",
    "\n",
    "def prepare_features(dataset):\n",
    "    from scipy.sparse import hstack\n",
    "    \n",
    "    #Usefull link https://www.dataquest.io/blog/natural-language-processing-with-python/\n",
    "    tokens = tokenize_sentenses(dataset['Headline'])\n",
    "    lemmas = lemmatize_tokens(tokens)\n",
    "    no_stop_words = remove_stopwords(lemmas)\n",
    "    pos_tags = no_stop_words.apply(encode_pos)\n",
    "    tf_idf = vectorizer.transform(dataset['Headline'])\n",
    "    #np.asmatrix(matrix.tolist())\n",
    "    embeddings = np.asmatrix(no_stop_words.apply(doc2vec).tolist())\n",
    "    \n",
    "    return hstack((tf_idf, embeddings)) \n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "vectorizer = trainTFIDF(train['Headline'], 2)\n",
    "matrix = prepare_features(train)\n",
    "train_labels = dense_to_one_hot(le.fit_transform(train['Stance']), 4)\n",
    "\n",
    "print (matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "w2vmodel = None\n",
    "vectorizer = None\n",
    "le = None\n",
    "wnl = None\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\anaconda3\\envs\\mlcv35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2252: UserWarning: Expected no kwargs, you passed 1\n",
      "kwargs passed to function are ignored with Tensorflow backend\n",
      "  warnings.warn('\\n'.join(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "39977/39977 [==============================] - 8s - loss: 0.7621 - acc: 0.7286     \n",
      "Epoch 2/15\n",
      "39977/39977 [==============================] - 7s - loss: 0.7185 - acc: 0.7295     \n",
      "Epoch 3/15\n",
      "39977/39977 [==============================] - 7s - loss: 0.7123 - acc: 0.7291     \n",
      "Epoch 4/15\n",
      "39977/39977 [==============================] - 7s - loss: 0.7118 - acc: 0.7295     \n",
      "Epoch 5/15\n",
      "39977/39977 [==============================] - 7s - loss: 0.7069 - acc: 0.7293     \n",
      "Epoch 6/15\n",
      "39977/39977 [==============================] - 7s - loss: 0.7084 - acc: 0.7279     \n",
      "Epoch 7/15\n",
      "39977/39977 [==============================] - 7s - loss: 0.7071 - acc: 0.7286     \n",
      "Epoch 8/15\n",
      "39977/39977 [==============================] - 7s - loss: 0.7108 - acc: 0.7273     \n",
      "Epoch 9/15\n",
      "39977/39977 [==============================] - 7s - loss: 0.7077 - acc: 0.7286     \n",
      "Epoch 10/15\n",
      "39977/39977 [==============================] - 7s - loss: 0.7100 - acc: 0.7295     \n",
      "Epoch 11/15\n",
      "39977/39977 [==============================] - 7s - loss: 0.7110 - acc: 0.7280     \n",
      "Epoch 12/15\n",
      "39977/39977 [==============================] - 7s - loss: 0.7102 - acc: 0.7294     \n",
      "Epoch 13/15\n",
      "39977/39977 [==============================] - 8s - loss: 0.7115 - acc: 0.7283     \n",
      "Epoch 14/15\n",
      "39977/39977 [==============================] - 7s - loss: 0.7134 - acc: 0.7278     \n",
      "Epoch 15/15\n",
      "39977/39977 [==============================] - 7s - loss: 0.7118 - acc: 0.7275     \n"
     ]
    }
   ],
   "source": [
    "def train_model(x_train, y_train):\n",
    "    from keras import metrics\n",
    "    \n",
    "    x_train = x_train.toarray()\n",
    "    input_size = x_train.shape[1]\n",
    "    output_size = 4\n",
    "    \n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout, Activation\n",
    "    from keras.optimizers import SGD\n",
    "\n",
    "    model = Sequential()\n",
    "    # Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "    # in the first layer, you must specify the expected input data shape:\n",
    "    # here, 20-dimensional vectors.\n",
    "    model.add(Dense(1024, activation='relu', input_dim=input_size))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax'))\n",
    "\n",
    "    #sgd = SGD(lr=1e-04, decay=2, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=\"rmsprop\",\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              epochs=15,\n",
    "              batch_size=128)\n",
    "#     score = model.evaluate(x_test, y_test, batch_size=16)\n",
    "#     print (score)\n",
    "    return model\n",
    "model = train_model(matrix, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39977/39977 [==============================] - 8s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.67397939837091925, 0.73041999150107362]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(matrix.toarray(), train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
